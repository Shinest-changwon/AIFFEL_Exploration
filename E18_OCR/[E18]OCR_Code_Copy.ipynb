{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "standard-drunk",
   "metadata": {},
   "source": [
    "## Google OCR API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-opportunity",
   "metadata": {},
   "source": [
    "### Step1. 구글의 파이썬 API 인터페이스 모듈을 설치  \n",
    "\n",
    "`$ pip install --upgrade google-api-python-client`  \n",
    "\n",
    "`$ pip install google-cloud-vision`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-pizza",
   "metadata": {},
   "source": [
    "### Step2. Google Cloud Vision API 사용\n",
    "아래 링크의 설명을 참고하여 서비스 계정 및 인증키를 생성합니다. 브라우저에서 다운로드한 인증키는 다음 경로에 my_google_api_key.json이라는 파일명으로 저장해 둡시다. (파일은 처음에 sheet-contents-로 시작되는 이름으로 자동 저장됩니다.)   \n",
    "\n",
    "[구글 클라우드 비전 API 사용하기](http://egloos.zum.com/mcchae/v/11342622)\n",
    "\n",
    "`$ cp ~/Downloads/sheet-contents-xxxx.json ~/aiffel/ocr_python/my_google_api_key.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-tragedy",
   "metadata": {},
   "source": [
    "### Step3. 인증키 경로 등록 후 커널 재기동  \n",
    "\n",
    "터미널을 열고 아래와 같이 인증키 경로 변수를 등록한 후 커널을 종료하고 재기동합니다.  \n",
    "`$ export GOOGLE_APPLICATION_CREDENTIALS=$HOME/aiffel/ocr_python/my_google_api_key.json`  \n",
    "\n",
    "만약 구글 API를 계속 사용하고 싶다면 아래와 같이 환경변수에 등록해 주면 위와 같이 매번 경로 변수 설정을 하지 않아도 됩니다.  \n",
    "  \n",
    "`$ echo \"export GOOGLE_APPLICATION_CREDENTIALS=$HOME/aiffel/ocr_python/my_google_api_key.json\" >> ~/.bashrc`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-copyright",
   "metadata": {},
   "source": [
    "### Step4. API 사용 테스트\n",
    "API를 활용하는 코드는 아래와 같습니다. 화면에서 테스트할 때 사용했던 이미지를 다시한번 사용해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(path):\n",
    "    \"\"\"Detects text in the file.\"\"\"\n",
    "    from google.cloud import vision\n",
    "    import io\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "        \n",
    "    image = vision.Image(content=content)\n",
    "\n",
    "    response = client.text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "    print('Texts:')\n",
    "\n",
    "    for text in texts:\n",
    "       print('\\n\"{}\"'.format(text.description))\n",
    "\n",
    "    vertices = (['({},{})'.format(vertex.x, vertex.y)\n",
    "                 for vertex in text.bounding_poly.vertices])\n",
    "\n",
    "    print('bounds: {}'.format(','.join(vertices)))\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            '{}\\nFor more info on error messages, check: '\n",
    "            'https://cloud.google.com/apis/design/errors'.format(\n",
    "                response.error.message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "patent-drove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 aiffel-dj40 aiffel-dj40 2355  3월 16 14:10 /home/aiffel-dj40/aiffel/ocr_python/my_google_api_key.json\n",
      "Texts:\n",
      "\n",
      "\"012345678\n",
      "SABCDEFGH\n",
      "IJKLMNOPQ\n",
      "RSTUVWXYZ\n",
      "012345678\n",
      "9ABCDEFGH\n",
      "IJKLMNOPO\n",
      "RSTUVWXYZ\n",
      "012345678\n",
      "9ABCDEFGH\n",
      "IJKLMNOP@\n",
      "R\n",
      "\"\n",
      "\n",
      "\"012345678\"\n",
      "\n",
      "\"SABCDEFGH\"\n",
      "\n",
      "\"IJKLMNOPQ\"\n",
      "\n",
      "\"RSTUVWXYZ\"\n",
      "\n",
      "\"012345678\"\n",
      "\n",
      "\"9ABCDEFGH\"\n",
      "\n",
      "\"IJKLMNOPO\"\n",
      "\n",
      "\"RSTUVWXYZ\"\n",
      "\n",
      "\"012345678\"\n",
      "\n",
      "\"9ABCDEFGH\"\n",
      "\n",
      "\"IJKLMNOP@\"\n",
      "\n",
      "\"R\"\n",
      "bounds: (5,555),(44,555),(44,594),(5,594)\n"
     ]
    }
   ],
   "source": [
    "# 다운받은 인증키 경로가 정확하게 지정되어 있어야 합니다. \n",
    "!ls -l $GOOGLE_APPLICATION_CREDENTIALS\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] =  os.getenv('HOME')+'/aiffel/ocr_python/my_google_api_key.json'\n",
    "\n",
    "# 입력 이미지 경로를 지정해 주세요.\n",
    "# (예시) path = os.getenv('HOME')+'/aiffel/ocr_python/test_image.png'\n",
    "path = os.getenv(\"HOME\") +'/1.AIFFEL_Study/Exploration/E18_OCR/pics/ocr1.png'\n",
    "\n",
    "# 위에서 정의한 OCR API 이용 함수를 호출해 봅시다.\n",
    "detect_text(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-network",
   "metadata": {},
   "source": [
    "이전에 구글 API 계정을 생성한 후 결제정보가 만료된 경우에는 위의 OCR API 호출시 billing 관련 오류가 날 수 있습니다. 오류메시지 아래쪽의 링크를 따라가서 결제계정을 재활성화해야 API 호출이 가능합니다.\n",
    "혹은 다른 계정을 생성하고 인증키를 다시 받는 방법도 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-resolution",
   "metadata": {},
   "source": [
    "## keras-ocr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-tribune",
   "metadata": {},
   "source": [
    "keras-ocr은 텐서플로우의 케라스 API를 기반으로 이미지 속 문자를 읽는 End-to-End OCR을 할 수 있게 해줍니다. 공식 문서에도 나와 있듯, 검출 모델로는 네이버 데뷰 2018 영상에서 소개한 CRAFT(Character Region Awareness for Text Detection)를 사용하고, 인식 모델로는 앞에서 설명한 CRNN을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-cambodia",
   "metadata": {},
   "source": [
    "주의 keras-ocr 은 tensorflow 버전 2.2.0 에서 구동됩니다. 2.3.X 이상 버전에서는 미리 학습된 모델에서 오류가 발생할 수 있으니 주의하세요. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-original",
   "metadata": {},
   "source": [
    "`$ pip list | grep tensorflow`    \n",
    "\n",
    "만약 tensorflow 버전이 맞지 않다면 재설치를 해줍시다.  \n",
    "`$ pip uninstall tensorflow`  \n",
    "\n",
    "`$ pip install tensorflow==2.2.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-reminder",
   "metadata": {},
   "source": [
    "이제 `keras-ocr`사용을 위해서 설치  \n",
    "`$ pip install keras-ocr`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-literature",
   "metadata": {},
   "source": [
    "필요한 라이브러리인 `keras_ocr` 과 인식결과의 시각화를 위한 `matplotlib.pyplot` 를 불러옵니다. `keras_ocr.pipeline.Pipeline()` 는 인식을 위한 파이프라인을 생성하는데 이때 초기화 과정에서 미리 학습된 모델의 가중치(weight)를 불러오게 됩니다. 검출기와 인식기를 위한 가중치 하나씩을 불러오겠지요.  \n",
    "\n",
    "[keras-ocr 공식문서](https://keras-ocr.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-windows",
   "metadata": {},
   "source": [
    "❗️ 아래 과정은 GPU를 사용할 수 있습니다. 평소 GPU 사용 시 cuDNN 관련 에러가 있었다면 아래 명령어를 사용해 보세요. export TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-ballot",
   "metadata": {},
   "source": [
    "(편집자 주) AIFFEL 학습환경 설치 과정에서 위 옵션이 아래와 같이 환경설정에 반영되어 있습니다.  \n",
    "`$ echo \"export TF_FORCE_GPU_ALLOW_GROWTH=true\" >> ~/.bashrc`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-emergency",
   "metadata": {},
   "source": [
    "확인을 위해서 터미널을 열어 다음과 같이 확인해 보시기 바랍니다. `true`가 출력되어야 환경설정에 반영되어 있는 것입니다. 만약 이 환경설정이 반영되어 있지 않으면 이후 코드 구동 과정에서 OOM(Out Of Memory) 에러가 날 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-sheffield",
   "metadata": {},
   "source": [
    "`$ echo $TF_FORCE_GPU_ALLOW_GROWTH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-reaction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for /home/aiffel-dj40/.keras-ocr/craft_mlt_25k.h5\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras_ocr\n",
    "\n",
    "# keras-ocr이 detector과 recognizer를 위한 모델을 자동으로 다운로드받게 됩니다. \n",
    "pipeline = keras_ocr.pipeline.Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-listing",
   "metadata": {},
   "source": [
    "만들어둔 파이프라인의 `recognize()` 에 이미지를 몇 개 넣어줍니다.\n",
    "이미지소스의 url을 사용할건데요. 이미지는 https://unsplash.com/s/photos/text 에서 가져왔습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dried-poster",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[1,64,920,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_1/basenet.slice1.0/Conv2D (defined at /home/aiffel-dj40/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/keras_ocr/detection.py:682) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_predict_function_6881]\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5c7da7fc62f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mkeras_ocr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_urls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprediction_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_urls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-5c7da7fc62f6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mkeras_ocr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_urls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprediction_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_urls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/keras_ocr/pipeline.py\u001b[0m in \u001b[0;36mrecognize\u001b[0;34m(self, images, detection_kwargs, recognition_kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecognition_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mrecognition_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mbox_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdetection_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         prediction_groups = self.recognizer.recognize_from_boxes(images=images,\n\u001b[1;32m     57\u001b[0m                                                                  \u001b[0mbox_groups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbox_groups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/keras_ocr/detection.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(self, images, detection_threshold, text_threshold, link_threshold, size_threshold, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \"\"\"\n\u001b[1;32m    681\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompute_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m         boxes = getBoxes(self.model.predict(np.array(images), **kwargs),\n\u001b[0m\u001b[1;32m    683\u001b[0m                          \u001b[0mdetection_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetection_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                          \u001b[0mtext_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1266\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[1,64,920,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_1/basenet.slice1.0/Conv2D (defined at /home/aiffel-dj40/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/keras_ocr/detection.py:682) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_predict_function_6881]\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "# 테스트에 사용할 이미지 url을 모아 봅니다. 추가로 더 모아볼 수도 있습니다. \n",
    "image_urls = [\n",
    "  'https://source.unsplash.com/M7mu6jXlcns/640x460',\n",
    "  'https://source.unsplash.com/6jsp4iHc8hI/640x460',\n",
    "  'https://source.unsplash.com/98uYQ-KupiE',\n",
    "  'https://source.unsplash.com/j9JoYpaJH3A',\n",
    "  'https://source.unsplash.com/eBkEJ9cH5b4'\n",
    "]\n",
    "\n",
    "images = [ keras_ocr.tools.read(url) for url in image_urls]\n",
    "prediction_groups = [pipeline.recognize([url]) for url in image_urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-design",
   "metadata": {},
   "source": [
    "이제 인식된 결과를 pyplot으로 시각화를 해봅니다.\n",
    "\n",
    "사용이 매우 간단합니다! 내부적으로 `recognize()` 는 검출기와 인식기를 두고, 검출기로 바운딩 박스(bounding box, 문자가 있는 영역을 표시한 정보)를 검출한 뒤, 인식기가 각 박스로부터 문자를 인식하는 과정을 거치도록 합니다.  \n",
    "\n",
    "[kera-ocr 파이프라인](https://github.com/faustomorales/keras-ocr/blob/master/keras_ocr/pipeline.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions\n",
    "fig, axs = plt.subplots(nrows=len(images), figsize=(20, 20))\n",
    "for idx, ax in enumerate(axs):\n",
    "    keras_ocr.tools.drawAnnotations(image=images[idx], \n",
    "                                    predictions=prediction_groups[idx][0], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-montgomery",
   "metadata": {},
   "source": [
    "(주의사항)\n",
    "keras-ocr은 한글 데이터셋으로 훈련이 되어있지 않은 모델입니다. 한글 텍스트의 detection은 정상적으로 진행되더라도 recognition 결과가 엉뚱하게 나올 수 있음에 주의해 주세요.  \n",
    "[https://github.com/faustomorales/keras-ocr/issues/101](https://github.com/faustomorales/keras-ocr/issues/101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-corruption",
   "metadata": {},
   "source": [
    "## 테서렉트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-martin",
   "metadata": {},
   "source": [
    "이번에는 테서랙트(Tesseract) 라이브러리로 이미지에서 문자를 인식해 보겠습니다. 테서랙트는 구글에서 후원하는 OCR 오픈소스 라이브러리로 현재는 버전 4와 Tessearct.js등으로 확장되는 등 많은 곳에서 사용되고 있습니다. 버전 4에서는 LSTM이 엔진에 추가되었고 현재 한국어를 포함한 116 개 국어를 지원하고 있습니다.\n",
    "\n",
    "오픈소스라는 점은 여러분들이 원하는 프로젝트에 활용하기 쉽다는 것을 뜻하니, 직접 해 보면서 익혀두면 나중에 간단한 OCR 모델이 필요할 때 빠르게 활용할 수 있을 겁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-mexican",
   "metadata": {},
   "source": [
    "### Step1. 테서렉트 설치\n",
    "우선 우분투에서 실행할 경우 터미널에서 아래 코드를 사용해 테서랙트 관련 패키지들을 설치해 주세요. 혹시 다른 운영체제를 사용하실 경우에는 아래 Tesseract Install Guide를 참고해주세요.  \n",
    "[테서렉트 설치 가이드](https://github.com/tesseract-ocr/tesseract/wiki)  \n",
    "   \n",
    "$ sudo apt install tesseract-ocr  \n",
    "\n",
    "$ sudo apt install libtesseract-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-vehicle",
   "metadata": {},
   "source": [
    "### Step2. 테서렉트 파이썬 wrapper 설치  \n",
    "`Pytesseract`는 OS에 설치된 테서랙트를 파이썬에서 쉽게 사용할 수있도록 해주는 래퍼 라이브러리(wrapper library)입니다. 파이썬 내에서 컴퓨터에 설치된 테서랙트 엔진의 기능을 바로 쓸 수 있도록 해줍니다.\n",
    "\n",
    "- 참고: [tesseract](https://pypi.org/project/pytesseract/)  \n",
    "- 참고: [위키백과: 래퍼 라이브러리](https://ko.wikipedia.org/wiki/%EB%9E%98%ED%8D%BC_%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC)  \n",
    "\n",
    "$ pip install pytesseract  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-decimal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-electric",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-sauce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-census",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-capital",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "accepting-justice",
   "metadata": {},
   "source": [
    "### 테서렉트로 문자 검출하고 이미지 자르기(detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from pytesseract import Output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# OCR Engine modes(–oem):\n",
    "# 0 - Legacy engine only.\n",
    "# 1 - Neural nets LSTM engine only.\n",
    "# 2 - Legacy + LSTM engines.\n",
    "# 3 - Default, based on what is available.\n",
    "\n",
    "# Page segmentation modes(–psm):\n",
    "# 0 - Orientation and script detection (OSD) only.\n",
    "# 1 - Automatic page segmentation with OSD.\n",
    "# 2 - Automatic page segmentation, but no OSD, or OCR.\n",
    "# 3 - Fully automatic page segmentation, but no OSD. (Default)\n",
    "# 4 - Assume a single column of text of variable sizes.\n",
    "# 5 - Assume a single uniform block of vertically aligned text.\n",
    "# 6 - Assume a single uniform block of text.\n",
    "# 7 - Treat the image as a single text line.\n",
    "# 8 - Treat the image as a single word.\n",
    "# 9 - Treat the image as a single word in a circle.\n",
    "# 10 - Treat the image as a single character.\n",
    "# 11 - Sparse text. Find as much text as possible in no particular order.\n",
    "# 12 - Sparse text with OSD.\n",
    "# 13 - Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific.\n",
    "\n",
    "def crop_word_regions(image_path='./images/sample.png', output_path='./output'):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)\n",
    "    custom_oem_psm_config = r'--oem 3 --psm 3'\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    recognized_data = pytesseract.image_to_data(\n",
    "        image, lang='kor',    # 한국어라면 lang='kor'\n",
    "        config=custom_oem_psm_config,\n",
    "        output_type=Output.DICT\n",
    "    )\n",
    "    \n",
    "    top_level = max(recognized_data['level'])\n",
    "    index = 0\n",
    "    cropped_image_path_list = []\n",
    "    for i in range(len(recognized_data['level'])):\n",
    "        level = recognized_data['level'][i]\n",
    "    \n",
    "        if level == top_level:\n",
    "            left = recognized_data['left'][i]\n",
    "            top = recognized_data['top'][i]\n",
    "            width = recognized_data['width'][i]\n",
    "            height = recognized_data['height'][i]\n",
    "            \n",
    "            output_img_path = os.path.join(output_path, f\"{str(index).zfill(4)}.png\")\n",
    "            print(output_img_path)\n",
    "            cropped_image = image.crop((\n",
    "                left,\n",
    "                top,\n",
    "                left+width,\n",
    "                top+height\n",
    "            ))\n",
    "            cropped_image.save(output_img_path)\n",
    "            cropped_image_path_list.append(output_img_path)\n",
    "            index += 1\n",
    "    return cropped_image_path_list\n",
    "\n",
    "\n",
    "work_dir = os.getenv('HOME')+'/aiffel/ocr_python'\n",
    "img_file_path = work_dir + '/test_image.png'   #테스트용 이미지 경로입니다. 본인이 선택한 파일명으로 바꿔주세요. \n",
    "\n",
    "cropped_image_path_list = crop_word_regions(img_file_path, work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-programmer",
   "metadata": {},
   "source": [
    "위에서 구현한 crop_word_regions() 함수는 여러분이 선택한 테스트 이미지를 받아서, 문자 검출을 진행한 후, 검출된 문자 영역을 crop한 이미지로 만들어 그 파일들의 list를 리턴하는 함수입니다.\n",
    "\n",
    "기본적으로 pytesseract.image_to_data() 를 사용합니다. 파이썬에서 편하게 사용하기 위해서 pytesseract 의 Output 을 사용해서 결과값의 형식을 딕셔너리(DICT) 형식으로 설정해주게 됩니다. 이렇게 인식된 결과는 바운딩 박스의 left, top, width, height 정보를 가지게 됩니다. 바운딩 박스를 사용해 이미지의 문자 영역들을 파이썬 PIL(pillow) 또는 opencv 라이브러리를 사용해 잘라(crop)서 cropped_image_path_list에 담아 리턴하였습니다.\n",
    "\n",
    "(주의) 위 코드에서 lang='kor' 로 바꾸면 에러가 발생합니다. 테서랙트의 언어팩을 설치해야 정상동작하게 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-monte",
   "metadata": {},
   "source": [
    "$ sudo apt install tesseract-ocr-kor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-latex",
   "metadata": {},
   "source": [
    "### 테서렉트로 잘린 이미지에서 단어 인식하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-belly",
   "metadata": {},
   "source": [
    "이제 문자 인식을 해 볼 차례입니다. 검출된 바운딩 박스 별로 잘린 이미지를 넣어주면 영역별 텍스트가 결과값으로 나오는 image_to_string()를 사용하게 됩니다.\n",
    "\n",
    "이렇게 인식된 결과가 실제 이미지와 맞는지 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_images(cropped_image_path_list):\n",
    "    custom_oem_psm_config = r'--oem 3 --psm 7'\n",
    "    \n",
    "    for image_path in cropped_image_path_list:\n",
    "        image = Image.open(image_path)\n",
    "        recognized_data = pytesseract.image_to_string(\n",
    "            image, lang='eng',    # 한국어라면 lang='kor'\n",
    "            config=custom_oem_psm_config,\n",
    "            output_type=Output.DICT\n",
    "        )\n",
    "        print(recognized_data['text'])\n",
    "    print(\"Done\")\n",
    "\n",
    "# 위에서 준비한 문자 영역 파일들을 인식하여 얻어진 텍스트를 출력합니다.\n",
    "recognize_images(cropped_image_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-award",
   "metadata": {},
   "source": [
    "블러처리  \n",
    "https://whereisend.tistory.com/143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-startup",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
